{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1zfB0xt7R50UwfQWmQ_MrCdXyrleweYt0","timestamp":1686462432513}],"authorship_tag":"ABX9TyNJSX/7azH6fB8vPyPGnMyJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install torchtext"],"metadata":{"id":"hAhJVS5ZUlKN"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E2goXloaj9un"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchtext.datasets import Multi30k\n","from torchtext.data import Field, BucketIterator\n","import spacy\n","import numpy as np\n","import random\n","import math\n","import time"]},{"cell_type":"code","source":["# Seeding\n","SEED = 100\n","random.seed(SEED)\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","torch.cuda.manual_seed(SEED)\n","torch.backends.cudnn.deterministic = True"],"metadata":{"id":"wHGbIsUaM3y3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Downloading SpaCy's vocabulary\n","!python -m spacy download en_core_web_sm\n","!python -m spacy download de_core_news_sm"],"metadata":{"id":"bouLPVYOM3s0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["spacy_de = spacy.load('de_core_news_sm')\n","spacy_en = spacy.load('en_core_web_sm')"],"metadata":{"id":"NfBfn4uSM35h"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aRZpdIfnj9us"},"outputs":[],"source":["#Tokenizing German text and reversing it for better result using the slicing operation(As mentioned in the seq2seq paper)\n","def tokenize_de(text): \n","    return [tok.text for tok in spacy_de.tokenizer(text)][::-1]\n","\n","#Tokenizing English text\n","def tokenize_en(text):\n","    return [tok.text for tok in spacy_en.tokenizer(text)]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"smVFemA9j9ut"},"outputs":[],"source":["#Assigning the sos and eos tokens and converting all words to lowercase\n","SR = Field(tokenize = tokenize_de, init_token = '<sos>', eos_token = '<eos>', lower = True)\n","\n","TR = Field(tokenize = tokenize_en, init_token = '<sos>', eos_token = '<eos>', lower = True)\n","\n","#Downloading and loading the train, validation and test data from Multi30k Dataset where SR(source) is GERMAN and TR(target) is ENGLISH\n","train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'), fields = (SR, TR))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Px1m8uqOj9uw"},"outputs":[],"source":["#checking the no of examples\n","print(f\"Number of training examples: {len(train_data.examples)}\")\n","print(f\"Number of validation examples: {len(valid_data.examples)}\")\n","print(f\"Number of testing examples: {len(test_data.examples)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tD5UlQMYj9ux"},"outputs":[],"source":["#Building the vocabulary for the source and target languages\n","SR.build_vocab(train_data, min_freq = 2)\n","TR.build_vocab(train_data, min_freq = 2)\n","print(f\"Unique tokens in source (de) vocabulary: {len(SR.vocab)}\")\n","print(f\"Unique tokens in target (en) vocabulary: {len(TR.vocab)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gbnuNyAtj9uy"},"outputs":[],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","BATCH_SIZE = 128\n","\n","#Using BucketIterator as it creates batches in such a way \n","#that it minimizes the amount of padding in both the source and target sentences.\n","train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n","     (train_data, valid_data, test_data), batch_size = BATCH_SIZE, device = device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YB8ZYUscj9u0"},"outputs":[],"source":["# Creating the encoder module wherein for the forward method, we pass in the source sentence, src ,\n","# which is converted into dense vectors using the embedding layer, and then dropout is applied.\n","# These embeddings are then passed into the RNN. \n","\n","class Encoder(nn.Module):\n","    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n","        super().__init__()\n","        \n","        self.hid_dim = hid_dim\n","        self.n_layers = n_layers\n","        self.embedding = nn.Embedding(input_dim, emb_dim)\n","        \n","        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n","        \n","        self.dropout = nn.Dropout(dropout)\n","        \n","# The forward fn takes the src tensor as input,\n","# which represents a batch of input sequences where src = [src len, batch size]\n","\n","    def forward(self, src):\n","        \n","        embedded = self.dropout(self.embedding(src))\n","        \n","        outputs, (hidden, cell) = self.rnn(embedded)\n","             \n","        return hidden, cell"]},{"cell_type":"code","source":["# To verify the dimensions of our tensors\n","\n","# src = [src len, batch size]\n","print(\"src shape:\", src.size())\n","        \n","# embedded = [src len, batch size, emb dim]\n","print(\"embedded shape:\", embedded.size())\n","        \n","# outputs = [src len, batch size, hid dim * n directions]\n","print(\"outputs shape:\", outputs.size())\n","\n","# hidden = [n layers * n directions, batch size, hid dim]\n","print(\"hidden shape:\", hidden.size())\n","\n","# cell = [n layers * n directions, batch size, hid dim]\n","print(\"cell shape:\", cell.size())"],"metadata":{"id":"EEElHBwm4vOw"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RWXFEtwEj9u1"},"outputs":[],"source":["class Decoder(nn.Module):\n","    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n","        super().__init__()\n","        \n","        self.output_dim = output_dim\n","        self.hid_dim = hid_dim\n","        self.n_layers = n_layers\n","        \n","        self.embedding = nn.Embedding(output_dim, emb_dim)\n","        \n","        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n","        \n","        self.fc_out = nn.Linear(hid_dim, output_dim)\n","        \n","        self.dropout = nn.Dropout(dropout)\n","        \n","    def forward(self, input, hidden, cell):\n","        \n","        # If not unsqueezed, the decoder would treat each token separately as a sequence and\n","        # perform the decoding operations independently for each token, which we dont want\n","        \n","        input = input.unsqueeze(0)\n","        \n","        embedded = self.dropout(self.embedding(input))\n","             \n","        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n","\n","        prediction = self.fc_out(output.squeeze(0))        \n","        \n","        return prediction, hidden, cell"]},{"cell_type":"code","source":[],"metadata":{"id":"LuVR-xx48Ogo"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VNcAOG57j9u2"},"outputs":[],"source":["class Seq2Seq(nn.Module):\n","    def __init__(self, encoder, decoder, device):\n","        super().__init__()\n","        \n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.device = device\n","        \n","        #Encoder and decoder must have equal number of layers and hidden dimensions\n","        assert encoder.hid_dim == decoder.hid_dim,\n","        assert encoder.n_layers == decoder.n_layers,\n","\n","    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n","        \n","        #src = [src len, batch size]\n","        #trg = [trg len, batch size]\n","        #teacher_forcing_ratio is probability to use teacher forcing\n","        #if teacher_forcing_ratio is 0.5 we use ground-truth inputs 50% of the time\n","        \n","        batch_size = trg.shape[1]\n","        trg_len = trg.shape[0]\n","        trg_vocab_size = self.decoder.output_dim\n","        \n","        #tensor to store decoder outputs\n","        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n","        \n","        #last hidden state of the encoder is used as the initial hidden state of the decoder\n","        hidden, cell = self.encoder(src)\n","        \n","        #first input to the decoder is the <sos> token\n","        input = trg[0,:]\n","        \n","        for t in range(1, trg_len):\n","            \n","            #insert input token embedding, previous hidden and previous cell states\n","            #receive output tensor (predictions) and new hidden and cell states\n","            output, hidden, cell = self.decoder(input, hidden, cell)\n","            \n","            #place predictions in a tensor holding predictions for each token\n","            outputs[t] = output\n","            \n","            #decide if we are going to use teacher forcing or not\n","            teacher_force = random.random() < teacher_forcing_ratio\n","            \n","            #get the highest predicted token from our predictions\n","            apex = output.argmax(1) \n","            \n","            #if teacher forcing, use actual next token as next input\n","            #if not, use predicted token\n","            input = trg[t] if teacher_force else apex\n","        \n","        return outputs"]},{"cell_type":"markdown","source":["Training the Seq2Seq Model"],"metadata":{"id":"GVuqnFHQ-uuy"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"7LnWapOWj9u3"},"outputs":[],"source":["#initializing our model\n","INPUT_DIM = len(SR.vocab)\n","OUTPUT_DIM = len(TR.vocab)\n","ENC_EMB_DIM = 256\n","DEC_EMB_DIM = 256\n","HID_DIM = 512\n","N_LAYERS = 2\n","ENC_DROPOUT = 0.5\n","DEC_DROPOUT = 0.5\n","\n","enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n","dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n","\n","model = Seq2Seq(enc, dec, device).to(device)\n","\n","#In the seq2seq paper they state that they initialize all weights from a uniform distribution between -0.08 and +0.08\n","def init_weights(m):\n","    for name, param in m.named_parameters():\n","        nn.init.uniform_(param.data, -0.08, 0.08)\n","        \n","model.apply(init_weights)\n","\n","#defining a function that will calculate the number of trainable parameters in the model\n","def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","print(f'The model has {count_parameters(model):,} trainable parameters')\n","\n","optimizer = optim.Adam(model.parameters())\n","\n","#ignoring the loss whenever the target token is a padding token by passing\n","# the index of the <pad> token as the ignore_index argument\n","TR_PAD_IDX = TR.vocab.stoi[TR.pad_token]\n","criterion = nn.CrossEntropyLoss(ignore_index = TR_PAD_IDX)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WRleb6iBj9vE"},"outputs":[],"source":["def train(model, iterator, optimizer, criterion, clip):\n","    \n","    #Differentiating between the training and evaluation modes as\n","    #dropout and batch normalization behave differently in the two modes\n","    model.train()\n","    \n","    epoch_loss = 0\n","    \n","    for i, batch in enumerate(iterator):\n","        \n","        src = batch.src\n","        trg = batch.trg\n","\n","        # Clear the accumulating gradients\n","        optimizer.zero_grad()\n","        \n","        output = model(src, trg)\n","\n","        #number of units dimensions in the output of the model        \n","        output_dim = output.shape[-1]\n","        \n","        #Removing sos token and reshaping the output tensor\n","        output = output[1:].view(-1, output_dim)\n","        trg = trg[1:].view(-1)\n","                \n","        # Calculate the loss value for every epoch\n","        loss = criterion(output, trg)\n","        \n","        # Calculate the gradients for weights & biases using back-propagation\n","        loss.backward()\n","        \n","        #Setting threshold value for gradients\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n","        \n","        optimizer.step()\n","        \n","        epoch_loss += loss.item()\n","        \n","    return epoch_loss / len(iterator)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zrL1klwvj9vF"},"outputs":[],"source":["def evaluate(model, iterator, criterion):\n","    \n","    model.eval()\n","    \n","    epoch_loss = 0\n","    \n","    with torch.no_grad():\n","    \n","        for i, batch in enumerate(iterator):\n","\n","            src = batch.src\n","            trg = batch.trg\n","\n","            #turning off teacher forcing\n","            output = model(src, trg, 0) \n","\n","            output_dim = output.shape[-1]\n","            \n","            output = output[1:].view(-1, output_dim)\n","            trg = trg[1:].view(-1)\n","\n","            loss = criterion(output, trg)\n","            \n","            epoch_loss += loss.item()\n","        \n","    return epoch_loss / len(iterator)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tk0ifMjbj9vG"},"outputs":[],"source":["def epoch_time(start_time, end_time):\n","    elapsed_time = end_time - start_time\n","    elapsed_mins = int(elapsed_time / 60)\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","    return elapsed_mins, elapsed_secs\n","\n","n_epoch = 10\n","clipp = 1\n","\n","#initialized with +ve infinite\n","best_valid_loss = float('inf')\n","\n","for epoch in range(n_epoch):\n","    \n","    start_time = time.time()\n","    \n","    train_loss = train(model, train_iterator, optimizer, criterion, clipp)\n","    valid_loss = evaluate(model, valid_iterator, criterion)\n","    \n","    end_time = time.time()\n","    \n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","    \n","    #updating the loss\n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save(model.state_dict(), 'seq2seqmodel_1.pt')\n","    \n","    print(f'Epoch: {epoch+1:02} \\n Time: {epoch_mins}m {epoch_secs}s')\n","    print(f'\\tTrain Loss: {train_loss:.3f} \\n Train PPL: {math.exp(train_loss):7.3f}')\n","    print(f'\\t Valid Loss: {valid_loss:.3f} \\n  Valid PPL: {math.exp(valid_loss):7.3f}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ofNcHGlPj9vH"},"outputs":[],"source":["#loading the saved parameters and evaluating the test dataset\n","model.load_state_dict(torch.load('seq2seqmodel_1.pt'))\n","test_loss = evaluate(model, test_iterator, criterion)\n","\n","print(f' Test Loss: {test_loss:.3f} \\n Test PPL: {math.exp(test_loss):7.3f} ')"]}]}