{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPAmSfcdMNkehyrOeQWbtrO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install torchtext==0.6.0 --quiet"],"metadata":{"id":"LTwXgs83oQ4I","executionInfo":{"status":"ok","timestamp":1686758307507,"user_tz":-330,"elapsed":5316,"user":{"displayName":"Aditya Chincholkar","userId":"07049589829618986463"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","execution_count":1,"metadata":{"id":"rKXzTvqK-SPF","executionInfo":{"status":"ok","timestamp":1686758213083,"user_tz":-330,"elapsed":20169,"user":{"displayName":"Aditya Chincholkar","userId":"07049589829618986463"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"854af5da-f222-4c50-ff5b-80af51358c96"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]}],"source":["import pandas as pd\n","import torch\n","import torch.nn as nn\n","import spacy\n","import nltk\n","nltk.download('punkt')  # Download required resource (only needed once)\n","from torchtext.datasets import Multi30k\n","from nltk.tokenize import word_tokenize\n","import numpy as np\n","import re"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QTxEdzB3iMAm","executionInfo":{"status":"ok","timestamp":1686758250660,"user_tz":-330,"elapsed":20077,"user":{"displayName":"Aditya Chincholkar","userId":"07049589829618986463"}},"outputId":"cd235c3b-df98-42a8-edcb-dd0ded163a99"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["from torchtext.data import TabularDataset, BucketIterator, Field"],"metadata":{"id":"8KTJMMzQ2_as","executionInfo":{"status":"error","timestamp":1686758259892,"user_tz":-330,"elapsed":6676,"user":{"displayName":"Aditya Chincholkar","userId":"07049589829618986463"}},"colab":{"base_uri":"https://localhost:8080/","height":373},"outputId":"cbb75fa1-92cf-4c58-fb9f-e146bc045f17"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]},{"output_type":"error","ename":"ImportError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-e0c58606adaa>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install torchtext==0.6.0 --quiet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTabularDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBucketIterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mField\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mImportError\u001b[0m: cannot import name 'TabularDataset' from 'torchtext.data' (/usr/local/lib/python3.10/dist-packages/torchtext/data/__init__.py)","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"markdown","source":["The dataset I chose is in the form of a sentence in english which is followed by it's translation on the same line seperated by '\\t'. Then on the same line some attributes are given before the '\\n' char. This is the format of every line.\n","\n","So what I am trying to do is make 2 lists, One with the english sentences and the other with the corresponding french translations."],"metadata":{"id":"lsr6XNz46ztS"}},{"cell_type":"code","source":["data = open('/content/drive/MyDrive/AI_club_projects/eng-fr_dataset.txt').read().split('\\t' or '\\n')\n","\n","lines = 100000  #We will consider first 1000 lines during training of model currently. we'll se afterwards what to do\n","eng_part = []\n","fr_part = []\n","\n","for i in range(1,lines):\n","  text = data[2*i]\n","  data[2*i] = text.split('\\n')[1]\n","\n","# JUST WRITING FOR REFERENCE\n","# If their are more than one \\n characters here then\n","# ---> [2] will mean give the part after 2nd \\n character\n","\n","\n","for i in range(0,lines):\n","  if (i%2==0):\n","    eng_part.append(data[i])\n","  else:\n","    fr_part.append(data[i])\n","\n","print(eng_part[7899:7999])\n","print(fr_part[7899:7999])\n","# Just printing to check the datasets\n","# Now we have the arrays with englich phrases/ sentences and corresponding french datasets"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9zlccRT_Xbpz","executionInfo":{"status":"ok","timestamp":1686746519868,"user_tz":-330,"elapsed":2783,"user":{"displayName":"Aditya Chincholkar","userId":"07049589829618986463"}},"outputId":"3e1e3478-819c-4170-9a79-1e55a4ef766f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['How did you do?', 'How disgusting!', 'How do I do it?', 'How do we look?', 'How do we look?', 'How exhausting!', 'How high is it?', 'How high is it?', 'How is Tom now?', 'How is it made?', 'How is it made?', 'How is it made?', 'How late is it?', 'How long is it?', 'How lucky I am!', 'How much is it?', 'How old is Tom?', 'How perceptive!', 'How tall is he?', 'How time flies!', 'How was Boston?', 'How wide is it?', \"How's the wife?\", \"How's your dad?\", \"How's your dad?\", \"How's your dad?\", 'Hurry up, guys.', 'I accept gifts.', 'I accept gifts.', 'I almost cried.', 'I already know.', 'I already know.', 'I am Hungarian.', 'I am Hungarian.', 'I am a redhead.', 'I am a shy boy.', 'I am a student.', 'I am a student.', 'I am a student.', 'I am a teacher.', 'I am a teacher.', 'I am an artist.', 'I am an artist.', 'I am in London.', 'I am in London.', 'I am off today.', 'I am too short.', 'I am very tall.', 'I ate a Danish.', 'I ate a banana.', 'I ate an apple.', 'I ate too much.', 'I became angry.', 'I became angry.', 'I began to cry.', 'I believed Tom.', 'I believed you.', 'I believed you.', 'I believed you.', 'I bet it works.', 'I bet it works.', 'I betrayed you.', 'I betrayed you.', 'I betrayed you.', 'I bike to work.', 'I bike to work.', 'I blame myself.', 'I borrow money.', 'I bought a car.', 'I bought a car.', 'I bought a hat.', 'I broke my arm.', 'I broke my leg.', 'I came for Tom.', 'I came for Tom.', 'I came for Tom.', 'I came for you.', 'I came for you.', 'I can beat Tom.', 'I can fix that.', 'I can fix this.', 'I can fix this.', 'I can hear you.', 'I can prove it.', \"I can't get in.\", \"I can't get up.\", \"I can't see it.\", \"I can't see it.\", \"I can't use it.\", \"I can't use it.\", 'I changed that.', 'I cook for Tom.', 'I count on Tom.', 'I deserve more.', 'I despised Tom.', 'I did warn you.', 'I did warn you.', 'I did warn you.', \"I didn't cheat.\", \"I didn't drive.\"]\n","['Wie hast du abgeschnitten?', 'Wie ekelhaft!', 'Wie mache ich das?', 'Wie sieht es für uns aus?', 'Was machen wir für einen Eindruck?', 'Wie anstrengend!', 'Wie hoch ist es?', 'Welche Höhe hat es?', 'Wie geht es Tom jetzt?', 'Wie wird es gemacht?', 'Wie wird sie gemacht?', 'Wie wird er gemacht?', 'Wie spät ist es?', 'Wie lang ist es?', 'Was habe ich für ein Glück!', 'Wie viel kostet es?', 'Wie alt ist Tom?', 'Wie scharfsinnig!', 'Wie groß ist er?', 'Wie die Zeit vergeht.', 'Wie war Boston?', 'Wie breit ist es?', 'Wie gehts der Frau?', 'Wie geht es deinem Vater?', 'Wie geht es eurem Vater?', 'Wie geht es Ihrem Vater?', 'Beeilt euch, Leute!', 'Ich nehme Geschenke entgegen.', 'Ich nehme Geschenke an.', 'Ich hätte fast geheult.', 'Das weiß ich schon.', 'Das ist mir bereits bekannt.', 'Ich bin Ungar.', 'Ich bin Ungarin.', 'Ich bin ein Rotschopf.', 'Ich bin ein schüchterner Junge.', 'Ich bin ein Student.', 'Ich bin Student.', 'Ich bin Studentin.', 'Ich bin Lehrer.', 'Ich bin Lehrerin.', 'Ich bin ein Künstler.', 'Ich bin Künstler.', 'Ich bin in London.', 'Ich befinde mich in London.', 'Ich habe heute frei.', 'Ich bin zu klein.', 'Ich bin sehr groß.', 'Ich habe ein Teilchen gegessen.', 'Ich habe eine Banane gegessen.', 'Ich habe einen Apfel gegessen.', 'Ich habe zu viel gegessen.', 'Ich wurde wütend.', 'Ich wurde böse.', 'Ich fing an zu weinen.', 'Ich habe Tom geglaubt.', 'Ich habe dir geglaubt.', 'Ich glaubte Ihnen.', 'Ich habe Ihnen geglaubt.', 'Ich wette, es funktioniert.', 'Ich wette, es klappt.', 'Ich habe dich verraten.', 'Ich habe Sie verraten.', 'Ich verriet Sie.', 'Ich fahre mit dem Fahrrad zur Arbeit.', 'Ich fahre mit dem Rad zur Arbeit.', 'Ich mache mich selbst dafür verantwortlich.', 'Ich leihe mir Geld.', 'Ich habe ein Auto gekauft.', 'Ich habe mir ein Auto gekauft.', 'Ich kaufte einen Hut.', 'Ich brach mir den Arm.', 'Ich habe mir das Bein gebrochen.', 'Ich kam wegen Tom.', 'Ich bin für Tom gekommen.', 'Ich bin statt Tom gekommen.', 'Ich bin wegen dir gekommen.', 'Ich bin deinetwegen gekommen.', 'Ich kann Tom schlagen.', 'Ich kann das reparieren.', 'Ich kann das reparieren.', 'Ich kann das beheben.', 'Ich kann dich hören.', 'Ich kann es beweisen.', 'Ich komme nicht hinein.', 'Ich kann nicht aufstehen.', 'Ich kann ihn nicht sehen.', 'Ich kann sie nicht sehen.', 'Ich habe keine Verwendung dafür.', 'Ich kann es nicht gebrauchen.', 'Ich ändere das.', 'Ich koche für Tom.', 'Ich zähle auf Tom.', 'Ich habe mehr verdient.', 'Ich habe Tom verachtet.', 'Ich habe dich ja gewarnt.', 'Ich habe euch ja gewarnt.', 'Ich habe Sie ja gewarnt.', 'Ich habe nicht geschummelt.', 'Ich bin nicht gefahren.']\n"]}]},{"cell_type":"code","source":["# Just representing the data in a tabular format to look better\n","raw_data = {'English': [line for line in eng_part],\n","            'French': [line for line in fr_part]}\n","\n","df = pd.DataFrame(raw_data , columns=['English' , 'French'])\n","\n","print(df)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UktnVTBORsKx","executionInfo":{"status":"ok","timestamp":1686746521703,"user_tz":-330,"elapsed":9,"user":{"displayName":"Aditya Chincholkar","userId":"07049589829618986463"}},"outputId":"bd62812b-9543-44ec-9b4e-10072079057c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["                      English                     French\n","0                         Go.                       Geh.\n","1                         Hi.                     Hallo!\n","2                         Hi.                 Grüß Gott!\n","3                        Run!                      Lauf!\n","4                        Run.                      Lauf!\n","...                       ...                        ...\n","49995  No one cares about me.        Ich bin allen egal.\n","49996  No one encouraged Tom.     Niemand bestärkte Tom.\n","49997  No one encouraged her.     Niemand ermutigte sie.\n","49998  No one has that right.  Niemand hat dieses Recht.\n","49999  No one has that right.  Dieses Recht hat niemand.\n","\n","[50000 rows x 2 columns]\n"]}]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","# create train and validation set\n","train, val = train_test_split(df, test_size=0.1)\n","train.to_csv(\"/content/drive/MyDrive/AI_club_projects/train.csv\", index=False)\n","val.to_csv(\"/content/drive/MyDrive/AI_club_projects/val.csv\", index=False)"],"metadata":{"id":"zxAAwWmMT80t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!python -m spacy download en_core_web_sm --quiet\n","!python -m spacy download de_core_news_sm --quiet\n","\n","# Spacy is a popular library for natural language processing (NLP) in Python. It provides various functionalities for tasks such as tokenization,\n","#part-of-speech tagging, named entity recognition, and dependency parsing.\n","#en--> english\n","# de--> german"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xrTdTud9kLIy","executionInfo":{"status":"ok","timestamp":1686746558818,"user_tz":-330,"elapsed":29283,"user":{"displayName":"Aditya Chincholkar","userId":"07049589829618986463"}},"outputId":"491e741b-3af0-4117-867b-805b55bf9742"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2023-06-14 12:42:12.600716: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m74.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_sm')\n","2023-06-14 12:42:27.595027: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('de_core_news_sm')\n"]}]},{"cell_type":"markdown","source":["Now to tokenise the data\n","\n","I know most of the initial lines are 1-2 words only but later on there are sentences."],"metadata":{"id":"BIOvezeq6gEK"}},{"cell_type":"code","source":["# This cell is what I did initially and was wrong. But still keeping this in comments\n","#eng_tokens1 = [re.sub(r'[^\\w\\s]', '', token) for token in eng_part if token]\n","#fr_tokens1 = [re.sub(r'[^\\w\\s]', '', token) for token in fr_part if token]\n","\n","#THE ABOVE STATEMENT WOULD REPLACE THE TOKENS.\n","\n","#eng_tokens = [word_tokenize(sentence) for sentence in eng_tokens1]\n","#fr_tokens = [word_tokenize(sentence) for sentence in fr_tokens1]\n","\n","#\n","\n","\n"],"metadata":{"id":"QgqL-y_YcQwm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["spacy_german = spacy.load(\"de_core_news_sm\")\n","spacy_english = spacy.load(\"en_core_web_sm\")\n","\n","def tokenize_german(text):\n","  return [token.text for token in spacy_german.tokenizer(text)]\n","\n","def tokenize_english(text):\n","  return [token.text for token in spacy_english.tokenizer(text)]\n","\n","from torchtext.data import Field\n","\n","german = Field(tokenize=tokenize_german, lower=True,\n","               init_token=\"<sos>\", eos_token=\"<eos>\")\n","\n","english = Field(tokenize=tokenize_english, lower=True,\n","               init_token=\"<sos>\", eos_token=\"<eos>\")"],"metadata":{"id":"riH5fZRVNAtl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["So at this point we have 2, 2D lists one having tokenised english sentences and the other having the corresponding french tokenised sentences. Here we have also removed all the punctuation marks to make our task easier."],"metadata":{"id":"Ht3WZw-Kd-z0"}},{"cell_type":"code","source":["data_fields = [('English', english), ('German', german)]\n","train,val = TabularDataset.splits(path=\"/content/drive/MyDrive/AI_club_projects/\", train='train.csv', test ='val.csv',format='csv',fields = data_fields)\n","# Here first we give the folder where to store these 2 files and then give them names.\n","\n","print(type(data_fields))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QhzjO6wLhmmZ","executionInfo":{"status":"ok","timestamp":1686746580597,"user_tz":-330,"elapsed":10566,"user":{"displayName":"Aditya Chincholkar","userId":"07049589829618986463"}},"outputId":"2f90d544-eacb-416d-9c30-17d17c65f26c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'list'>\n"]}]},{"cell_type":"code","source":["german.build_vocab(train, max_size=10000, min_freq=3)\n","english.build_vocab(train, max_size=10000, min_freq=3)\n","# Here we are building vocabulary.this scans data, maps tokens t unique integer IDs\n","\n","\n","print(f\"Unique tokens in source (de) vocabulary: {len(german.vocab)}\")\n","print(f\"Unique tokens in target (en) vocabulary: {len(english.vocab)}\")\n"],"metadata":{"id":"F1SJtHSwiKJE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1686746585506,"user_tz":-330,"elapsed":529,"user":{"displayName":"Aditya Chincholkar","userId":"07049589829618986463"}},"outputId":"6fde2d00-e60d-4ff2-a4e0-5003817e3a72"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Unique tokens in source (de) vocabulary: 4021\n","Unique tokens in target (en) vocabulary: 3037\n"]}]},{"cell_type":"code","source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","BATCH_SIZE = 32\n","train_iterator, valid_iterator = BucketIterator.splits((train,val),\n","                                                                     batch_size = BATCH_SIZE,\n","                                                                      sort_within_batch=True,\n","                                                                      sort_key=lambda x: len(x.src),\n","                                                                      device = device)\n","\n","# Here we are arranging the data based on the length of sentences\n","\n"],"metadata":{"id":"NoBZcT1GT14s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"X8JM_EILW7rl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"e8fYYbKGW74c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["NOW WE FINALLY COME TO THE MODEL.\n","We will go as follows\n","\n","-->Encoder LSTM\n","-->Decoder LSTM\n","-->Seq2Seq model\n","-->Then we will implement these models\n","-->Define a function to translate a given sentence\n","--> Define calculation of bleu score\n","-->Train the model\n","\n","\n","-->We'll see if theres anything else"],"metadata":{"id":"8zC1-rhzeCv3"}},{"cell_type":"code","source":["class EncoderLSTM(nn.Module):\n","  def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n","    super(EncoderLSTM, self).__init__()\n","\n","    # Size of the one hot vectors that will be the input to the encoder\n","    #self.input_size = input_size  (commented bcoz not used ahead anywhere)\n","\n","    # Output size of the word embedding NN\n","    #self.embedding_size = embedding_size\n","\n","    # Dimension of the NN's inside the lstm cell/ (hs,cs)'s dimension.\n","    self.hidden_size = hidden_size\n","\n","    # Number of layers in the lstm\n","    self.num_layers = num_layers\n","\n","    # Regularization parameter\n","    self.dropout = nn.Dropout(p)\n","    # Here this step we increase noise and hence reduce the problem of overfitting\n","    # This layer randomly sets\n","\n","    # The input size is the no. of words in the vocabulary learnt.\n","    # Enbedding size is the dimension of representation of each word in the vocab.\n","    # Generally higher value of enbedding size ight help to solve issues like:\n","    #   words may have diff meaning based on context. So for these things.\n","    self.embedding = nn.Embedding(input_size, embedding_size)\n","\n","    # we assign the layer here.\n","    # hidden_size = typically 100.  Here 300\n","    # num_layers = 2\n","    self.LSTM = nn.LSTM(embedding_size, hidden_size, num_layers, dropout = p)\n","\n","  # Shape of x (26, 32) [Sequence_length, batch_size]\n","  def forward(self, x):\n","\n","    # Shape -----------> (26, 32, 300) [Sequence_length , batch_size , embedding dims]\n","    embedding = self.dropout(self.embedding(x))\n","\n","    # Shape --> outputs (26, 32, 1024) [Sequence_length , batch_size , hidden_size]\n","    # Shape --> (hs, cs) (2, 32, 1024) , (2, 32, 1024) [num_layers, batch_size size, hidden_size]\n","    outputs, (hidden_state, cell_state) = self.LSTM(embedding)\n","\n","    return hidden_state, cell_state\n","\n","input_size_encoder = len(english.vocab)\n","encoder_embedding_size = 300\n","hidden_size = 1024\n","num_layers = 2\n","encoder_dropout = 0.5\n","\n","encoder_lstm = EncoderLSTM(input_size_encoder, encoder_embedding_size,\n","                           hidden_size, num_layers, encoder_dropout).to(device)\n","print(encoder_lstm)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UOo2Yp8peBiE","executionInfo":{"status":"ok","timestamp":1686746603210,"user_tz":-330,"elapsed":492,"user":{"displayName":"Aditya Chincholkar","userId":"07049589829618986463"}},"outputId":"388d4f3b-a951-4fe0-99d3-87c598194069"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["EncoderLSTM(\n","  (dropout): Dropout(p=0.5, inplace=False)\n","  (embedding): Embedding(3037, 300)\n","  (LSTM): LSTM(300, 1024, num_layers=2, dropout=0.5)\n",")\n"]}]},{"cell_type":"markdown","source":["The Dropout Layer\n","\n","Basically what it does is makes/ assigns some random neurons to be zero.\n","This helps the problem of overfitting and also, Allows it to be trained on a bit more noisy data\n","This also helps model to in a way learn the language instead of just mugging up the data\n","\n"],"metadata":{"id":"zc_EmY2ktQFo"}},{"cell_type":"code","source":["class DecoderLSTM(nn.Module):\n","  def __init__(self, input_size, embedding_size, hidden_size, num_layers, p, output_size):\n","    super(DecoderLSTM, self).__init__()\n","\n","    # Size of the one hot vectors that will be the input to the encoder\n","    #self.input_size = input_size  (commented becoz not used anywhere ahead)\n","\n","    # Output size of the word embedding NN\n","    self.embedding_size = embedding_size\n","\n","    # Dimension of the NN's inside the lstm cell/ (hs,cs)'s dimension.\n","    self.hidden_size = hidden_size\n","\n","    # Number of layers in the lstm\n","    self.num_layers = num_layers\n","\n","    # Size of the one hot vectors that will be the output to the encoder (English Vocab Size)\n","    self.output_size = output_size\n","\n","    # Regularization parameter\n","    self.dropout = nn.Dropout(p)\n","\n","    # The embedding layer\n","    self.embedding = nn.Embedding(input_size, embedding_size)\n","\n","    # Definig the lstm cell/layer. dim--> (300, 2, 1024) [embedding dims, hidden size, num layers]\n","    self.LSTM = nn.LSTM(embedding_size, hidden_size, num_layers, dropout = p)\n","\n","\n","    # This is the fully connected linear layer.\n","    # Here the training of wights and biasses os done\n","    # The hidden size and output size is taken as input to figure our shape of weight and bias matrices.\n","    # Shape -----------> (1024, 4556) [embedding dims, hidden size, num layers]\n","    self.fc = nn.Linear(hidden_size, output_size)\n","\n","  # Shape of x (32) [batch_size]\n","  def forward(self, x, hidden_state, cell_state):\n","\n","    # This unsqueeze will add another dimension to the tensor. This is done to make the processing consistent\n","    # The LSTM layer expects a 3-dimensional input tensor of shape==> batch_size, seq_length, input_size\n","    # Here x goes from(32) --> (1, 32) [batch_size]\n","    x = x.unsqueeze(0)\n","\n","    # Shape -----------> (1, 32, 300) [1, batch_size, embedding dims]\n","    embedding = self.dropout(self.embedding(x))\n","\n","    # Shape --> outputs (1, 32, 1024) [1, batch_size , hidden_size]\n","    # Shape --> (hs, cs) (2, 32, 1024) , (2, 32, 1024) [num_layers, batch_size size, hidden_size] (passing encoder's hs, cs - context vectors)\n","\n","    outputs, (hidden_state, cell_state) = self.LSTM(embedding, (hidden_state, cell_state))\n","\n","    # Shape --> predictions (1, 32, 4556) [ 1, batch_size , output_size]\n","    # The predictions would be outputs of matrix mulptiplication and bias addition.\n","# Note: The outputs of the decoder layer are trained using a fully connected linear layer.\n","    predictions = self.fc(outputs)\n","\n","    # Shape --> predictions (32, 4556) [batch_size , output_size]\n","    predictions = predictions.squeeze(0)\n","\n","    return predictions, hidden_state, cell_state\n","\n","input_size_decoder = len(german.vocab)\n","decoder_embedding_size = 300\n","hidden_size = 1024\n","num_layers = 2\n","decoder_dropout = 0.5\n","output_size = len(german.vocab)\n","\n","decoder_lstm = DecoderLSTM(input_size_decoder, decoder_embedding_size,\n","                           hidden_size, num_layers, decoder_dropout, output_size).to(device)\n","print(decoder_lstm)"],"metadata":{"id":"euDPysooiyKM","executionInfo":{"status":"ok","timestamp":1686746618132,"user_tz":-330,"elapsed":489,"user":{"displayName":"Aditya Chincholkar","userId":"07049589829618986463"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"4e624d07-814f-48b8-b321-6f6a7f0dff4f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["DecoderLSTM(\n","  (dropout): Dropout(p=0.5, inplace=False)\n","  (embedding): Embedding(4021, 300)\n","  (LSTM): LSTM(300, 1024, num_layers=2, dropout=0.5)\n","  (fc): Linear(in_features=1024, out_features=4021, bias=True)\n",")\n"]}]},{"cell_type":"markdown","source":["Some theory about the embedding Layer\n","\n","The nn.Embedding layer takes as input a tensor of indices representing discrete variables. The input tensor's shape can be (batch_size, sequence_length) if you're processing sequences. Each index corresponds to a discrete variable value, such as a word ID.\n","\n","It outputs a tensor of shape (batch_size, sequence_length, embedding_dim). Each element in the output tensor represents the dense embedding vector for the corresponding index in the input tensor."],"metadata":{"id":"51tVT9N6o9x0"}},{"cell_type":"code","source":["class Seq2Seq(nn.Module):\n","  def __init__(self, Encoder_LSTM, Decoder_LSTM):\n","    super(Seq2Seq, self).__init__()\n","    self.Encoder_LSTM = Encoder_LSTM\n","    self.Decoder_LSTM = Decoder_LSTM\n","    # Iheritance of the 2 classes, encoder and decoder\n","\n","  def forward(self, source, target, tfr=0.5):\n","    # Shape - Source : (10, 32) [(Sentence length English + some padding), Number of Sentences]\n","    batch_size = source.shape[1]\n","\n","    # Shape - Source : (14, 32) [(Sentence length German + some padding), Number of Sentences]\n","    target_len = target.shape[0]\n","    target_vocab_size = len(german.vocab)\n","\n","    # Shape --> outputs (14, 32, 5766)\n","    # Creating a tensor with zero values, of dimensions..(target_len, batch_size, target_vocab_size)\n","    outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n","\n","    # Shape --> (hs, cs) (2, 32, 1024) ,(2, 32, 1024) [num_layers, batch_size size, hidden_size] (contains encoder's hs, cs - context vectors)\n","    # The outputs of encoder lstm.\n","    hidden_state, cell_state = self.Encoder_LSTM(source)\n","\n","    # Shape of x (32 elements)\n","    x = target[0] # Trigger token <SOS>\n","\n","    for i in range(1, target_len):\n","\n","      # Shape --> output (32, 5766)\n","      output, hidden_state, cell_state = self.Decoder_LSTM(x, hidden_state, cell_state)\n","      outputs[i] = output\n","\n","    # Shape --> outputs (14, 32, 5766)\n","    return outputs"],"metadata":{"id":"JbxG06PvqOfM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.utils.tensorboard import SummaryWriter\n","import torch.optim as optim\n","from torch.utils.tensorboard import SummaryWriter\n","\n","#creating a writer obj\n","writer = SummaryWriter(log_dir='logs')"],"metadata":{"id":"j-t_4DXfrU3D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["learning_rate = 0.001\n","step = 0\n","\n","\n","model = Seq2Seq(encoder_lstm, decoder_lstm).to(device)\n","# Using adam optimiser\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","pad_idx = english.vocab.stoi[\"<pad>\"]\n","# Cross entorpy loss function\n","criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)"],"metadata":{"id":"vm7G9csMrVBf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model\n","# To check how is the model running, and check all the info about model\n","# This includes all our hyperparameters, the dimensions, layers of model etc."],"metadata":{"id":"diyQvueVrVMA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1686746653893,"user_tz":-330,"elapsed":507,"user":{"displayName":"Aditya Chincholkar","userId":"07049589829618986463"}},"outputId":"6b95dc01-464f-4936-d780-ccee128cc1fa"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Seq2Seq(\n","  (Encoder_LSTM): EncoderLSTM(\n","    (dropout): Dropout(p=0.5, inplace=False)\n","    (embedding): Embedding(3037, 300)\n","    (LSTM): LSTM(300, 1024, num_layers=2, dropout=0.5)\n","  )\n","  (Decoder_LSTM): DecoderLSTM(\n","    (dropout): Dropout(p=0.5, inplace=False)\n","    (embedding): Embedding(4021, 300)\n","    (LSTM): LSTM(300, 1024, num_layers=2, dropout=0.5)\n","    (fc): Linear(in_features=1024, out_features=4021, bias=True)\n","  )\n",")"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["def translate_sentence(model, sentence, german, english, device, max_length=50):\n","    spacy_en = spacy.load(\"en_core_web_sm\") # English is the input language\n","\n","    if type(sentence) == str:\n","        tokens = [token.text.lower() for token in spacy_en(sentence)]\n","    else:\n","        tokens = [token.lower() for token in sentence]\n","    tokens.insert(0, english.init_token)# Insert \"<sos>\"\n","    tokens.append(german.eos_token)# Insert \"<eos>\"\n","\n","    text_to_indices = [english.vocab.stoi[token] for token in tokens]\n","    # .stoi maps the tokens to corresponding integers.\n","    # Note text_to_indices is a list which is converted to a tensor\n","    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n","    #Here unsqueeze(1) is done to include the additional dimension which corresponds to the batch size.\n","\n","    # Build encoder hidden, cell state\n","    with torch.no_grad():\n","        hidden, cell = model.Encoder_LSTM(sentence_tensor)\n","\n","    outputs = [german.vocab.stoi[\"<sos>\"]]\n","    # The outputs list, with german translation.\n","\n","    for _ in range(max_length):\n","        previous_word = torch.LongTensor([outputs[-1]]).to(device)\n","        # This takes the last predicted word and converts it to a tensor\n","\n","        with torch.no_grad(): # Running the decoder step\n","            output, hidden, cell = model.Decoder_LSTM(previous_word, hidden, cell)\n","            best_guess = output.argmax(1).item()\n","\n","        outputs.append(best_guess)\n","        # This has the guesses made by the model in terms of the numerical IDs\n","\n","        # Model predicts it's the end of the sentence\n","        if output.argmax(1).item() == german.vocab.stoi[\"<eos>\"]:\n","            break\n","\n","    translated_sentence = [german.vocab.itos[idx] for idx in outputs]\n","    # This converts/ maps the IDs to the respective tokens ( .itos)\n","    return translated_sentence[1:]\n","    # Here we dont return the \"<sos>\" token"],"metadata":{"id":"fKG8-vM2WcOY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The bleu score is a performance metric which measures the quality of the translationThe BLEU score is based on the concept of precision, comparing the n-grams (contiguous sequences of n words) in the machine-generated translation to the n-grams in the reference translations. It considers both precision at the n-gram level and the brevity penalty to account for differences in length between the machine-generated and reference translations.\n","\n","Measures the number of phrases which are correct/ the words are same as expected( the phrases are of n words) where n-grams can be 1/2 so on"],"metadata":{"id":"zsbv9cJDeAAT"}},{"cell_type":"code","source":["from torchtext.data.metrics import bleu_score\n","\n","def bleu(data, model, german, english, device):\n","    targets = []\n","    outputs = []\n","\n","    for example in data:\n","        src = vars(example)[\"src\"]\n","        trg = vars(example)[\"trg\"]\n","\n","        prediction = translate_sentence(model, src, german, english, device)\n","        prediction = prediction[:-1]  # remove <eos> token\n","\n","        targets.append([trg])\n","        outputs.append(prediction)\n","\n","    return bleu_score(outputs, targets)\n","\n","# Just wanted to check something\n","print(bleu_score(['hi','hello','tom','yo','hey'], ['hi','hello','tom','yo','hey']))"],"metadata":{"id":"X7fQbJdxdFew","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1686746809028,"user_tz":-330,"elapsed":6,"user":{"displayName":"Aditya Chincholkar","userId":"07049589829618986463"}},"outputId":"7e0252a5-f0d3-4b78-9391-c84842de2ef9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.0\n"]}]},{"cell_type":"code","source":["def checkpoint_and_save(model, best_loss, epoch, optimizer, epoch_loss):\n","\n","    state = {'model': model,'best_loss': best_loss,'epoch': epoch,'rng_state': torch.get_rng_state(), 'optimizer': optimizer.state_dict(),}\n","    torch.save(state, '/content/checkpoint-NMT-SD')\n","    torch.save(model.state_dict(),'/content/checkpoint-NMT-SD')"],"metadata":{"id":"pDFQyKl65cD8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["epoch_loss = 0.0\n","num_epochs = 100\n","best_loss = 999999\n","best_epoch = -1\n","sentence1 = \"Tom you should go home right now\"\n","ts1  = []\n","\n","for epoch in range(num_epochs):\n","  print(\"Epoch - {} / {}\".format(epoch+1, num_epochs))\n","  model.eval()\n","  translated_sentence1 = translate_sentence(model, sentence1, german, english, device, max_length=15)\n","  # I know I will give a small sentence. so max_length only 15\n","  print(f\"Translated example sentence 1: \\n {translated_sentence1}\")\n","  ts1.append(translated_sentence1)\n","\n","  model.train(True)\n","  for batch_idx, batch in enumerate(train_iterator):\n","    input = batch.src.to(device)\n","    target = batch.trg.to(device)\n","\n","    # Pass the input and target for model's forward method\n","    output = model(input, target)\n","    output = output[1:].reshape(-1, output.shape[2])\n","    target = target[1:].reshape(-1)\n","\n","    # Clear the accumulating gradients\n","    optimizer.zero_grad()\n","\n","    # Calculate the loss value for every epoch\n","    loss = criterion(output, target)\n","\n","    # Calculate the gradients for weights & biases using back-propagation\n","    loss.backward()\n","\n","    # Clip the gradient value is it exceeds > 1\n","    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","\n","    # Update the weights values using the gradients we calculated using bp\n","    optimizer.step()\n","    step += 1\n","    epoch_loss += loss.item()\n","    writer.add_scalar(\"Training loss\", loss, global_step=step)\n","\n","  if epoch_loss < best_loss:\n","    best_loss = epoch_loss\n","    best_epoch = epoch\n","    checkpoint_and_save(model, best_loss, epoch, optimizer, epoch_loss)\n","    if ((epoch - best_epoch) >= 10):\n","      print(\"no improvement in 10 epochs, break\")\n","      break\n","  print(\"Epoch_Loss - {}\".format(loss.item()))\n","  print()\n","\n","print(epoch_loss / len(train_iterator))\n","\n","score = bleu(test_data[1:100], model, german, english, device)\n","print(f\"Bleu score {score*100:.2f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":452},"id":"g6p8q6fz6vr8","executionInfo":{"status":"error","timestamp":1686748479086,"user_tz":-330,"elapsed":1856,"user":{"displayName":"Aditya Chincholkar","userId":"07049589829618986463"}},"outputId":"9ee03d5e-dcbc-47b5-ea22-cc2ec4aeda13"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch - 1 / 100\n","Translated example sentence 1: \n"," ['abendbrot', 'tue', 'gibt', 'gibt', 'krawatte', 'lebend', 'lebend', 'gefüttert', 'wären', 'schwanger', 'schwanger', 'arzt', 'wandern', '2.30', 'kennst']\n"]},{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-37-b7fcab679ec7>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchtext/data/iterator.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminibatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m                 \u001b[0;31m# fast-forward if loaded from state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterations_this_epoch\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchtext/data/iterator.py\u001b[0m in \u001b[0;36mpool\u001b[0;34m(data, batch_size, key, batch_size_fn, random_shuffler, shuffle, sort_within_batch)\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0mrandom_shuffler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0mp_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msort_within_batch\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m             \u001b[0;32melse\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-12-d16211bb1d8a>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      4\u001b[0m                                                                      \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                                                       \u001b[0msort_within_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                                                                       \u001b[0msort_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m                                                                       device = device)\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'Example' object has no attribute 'src'"]}]}]}